{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "import itertools as it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Base Model\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for model training\n",
    "# Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataa=[]\n",
    "training_labell=[]\n",
    "\n",
    "window=30\n",
    "stride=int(window/2)  \n",
    "\n",
    "for k in range(7):\n",
    "    Files=[]\n",
    "    for file in sorted(glob.glob('./ALR_2/Copy_2/1.Distance/Testing/subject_1/Action_{}/*.txt'.format(k))):\n",
    "        Files.append(file)\n",
    "\n",
    "    for m in range(len(Files)):\n",
    "        if m not in (3,7,13,17,23,27,33,37,43,47,53,57,63,67,73,77,83,87,93,97):\n",
    "            f = open(Files[m], 'r')\n",
    "            df = f.readlines()\n",
    "            f.close()\n",
    "            p=[]\n",
    "            for i in range(len(df)):\n",
    "                temp = df[i].split()\n",
    "                p.append(temp)\n",
    "            p = np.array(p, 'float32')\n",
    "\n",
    "            for i in range(0,len(p)-stride, stride):\n",
    "                new_mat = p[i:i+window]\n",
    "                where_are_NaNs = np.isnan(new_mat)\n",
    "                new_mat[where_are_NaNs]=1\n",
    "                new_mat = (new_mat-np.mean(new_mat))/(np.max(new_mat)-np.min(new_mat))\n",
    "\n",
    "                training_dataa.append(new_mat)\n",
    "                training_labell.append(k)\n",
    "\n",
    "training_data = training_dataa\n",
    "training_label = training_labell\n",
    "\n",
    "print(len(training_dataa))\n",
    "print(len(training_labell))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For DISTANCE features\n",
    "training_data_dis = np.array(training_data,'float64')\n",
    "training_data_dis = training_data_dis.reshape(len(training_data_dis),row,column,1)\n",
    "\n",
    "training_label=np.array(training_label)\n",
    "print(training_data_dis.shape)\n",
    "print(training_label.shape)\n",
    "print(type(training_data_dis))\n",
    "print(type(training_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataa=[]\n",
    "testing_labell=[]\n",
    "\n",
    "window = 30\n",
    "stride = int(window/2)\n",
    "\n",
    "for k in range(7):\n",
    "    Files=[]\n",
    "    for file in sorted(glob.glob('./ALR_2/Copy_2/1.Distance/Testing/subject_1/Action_{}/*.txt'.format(k))):\n",
    "        Files.append(file)\n",
    "         \n",
    "    for m in range(len(Files)):\n",
    "        if m in (5,6,7,8,9,15,16,17,18,19):\n",
    "            f = open(Files[m], 'r')\n",
    "            df = f.readlines()\n",
    "            f.close()\n",
    "            p=[]\n",
    "            for i in range(len(df)):\n",
    "                temp=df[i].split()\n",
    "                p.append(temp)\n",
    "            p = np.array(p,'float32')\n",
    "            for i in range(0,len(p)-stride, stride):\n",
    "                new_mat = p[i:i+window]\n",
    "                where_are_NaNs = np.isnan(new_mat)\n",
    "                new_mat[where_are_NaNs]=1\n",
    "                new_mat = (new_mat-np.mean(new_mat))/(np.max(new_mat)-np.min(new_mat)) \n",
    "                testing_dataa.append(new_mat)\n",
    "                testing_labell.append(k)\n",
    "testing_data=testing_dataa          \n",
    "testing_label=testing_labell           \n",
    "\n",
    "print(len(testing_data))\n",
    "print(len(testing_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = len(testing_data[0])\n",
    "column = len(testing_data[0][0])\n",
    "print('row: ', row)\n",
    "print('column: ', column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distance Feature\n",
    "testing_data_dis = np.array(testing_data, 'float32')\n",
    "testing_data_dis = testing_data_dis.reshape((len(testing_data_dis),row,column,1))\n",
    "\n",
    "testing_label=np.array(testing_label)\n",
    "print(testing_data_dis.shape)\n",
    "print(testing_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the data labels to categorical\n",
    "training_label = keras.utils.to_categorical(training_label, 7)\n",
    "testing_label= keras.utils.to_categorical(testing_label, 7)\n",
    "\n",
    "# print(training_label.shape)\n",
    "print(testing_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The best model with 3D skeleton data \n",
    "row = row\n",
    "column = column\n",
    "\n",
    "kinect_input = Input(shape=(row, column, 1))\n",
    "\n",
    "x = Conv2D(64, (3,3), activation='relu',padding='same',kernel_initializer='he_normal', \\\n",
    "           kernel_regularizer=l2(0.001))(kinect_input)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Conv2D(64, (3,3), activation='relu',padding='same',kernel_initializer='he_normal', kernel_regularizer=l2(0.01))(x)\n",
    "x = MaxPooling2D(pool_size=(2,2))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Conv2D(128, (3,3), activation='relu',padding='same',kernel_initializer='he_normal', kernel_regularizer=l2(0.01))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Conv2D(128, (3,3), activation='relu',padding='same',kernel_initializer='he_normal', kernel_regularizer=l2(0.001))(x)\n",
    "x = MaxPooling2D(pool_size=(2,2))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.7)(x)\n",
    "predictions = Dense(7, activation = 'softmax')(x)\n",
    "\n",
    "base_model = Model(inputs = kinect_input, output = predictions)\n",
    "\n",
    "adam = keras.optimizers.Adam(lr= 0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "base_model.compile(loss='categorical_crossentropy', optimizer=adam,  metrics=['accuracy'])\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath,\n",
    "                            monitor='val_acc',\n",
    "                            verbose=1,\n",
    "                            save_best_only=True,\n",
    "                            mode='max')\n",
    "\n",
    "history = base_model.fit([training_data_dis], \n",
    "                         training_label,\n",
    "                batch_size=32,\n",
    "                epochs=100,\n",
    "                verbose=1,\n",
    "                validation_data=([testing_data_dis], testing_label),\n",
    "                callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.load_weights(filepath)\n",
    "test_pur = base_model.predict([testing_data_dis], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_1 = 0\n",
    "for i in range(len(testing_label)):\n",
    "    if np.argmax(testing_label[i]) == np.argmax(test_pur[i]):\n",
    "        match_1+=1\n",
    "accuracy_1 = (match_1/len(testing_label))*100\n",
    "print('left_hand accuracy :', accuracy_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(history.history['acc']))\n",
    "print(np.mean(history.history['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (history.history['acc'], history.history['val_acc'])\n",
    "for i in range(len(history.history['acc'])):\n",
    "    print(i,history.history['acc'][i],history.history['val_acc'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plotting the confusion metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "for i in range(len(testing_label)):\n",
    "    y = np.argmax(testing_label[i])\n",
    "    y_true.append(y)\n",
    "\n",
    "for i in range(len(test_pur)):\n",
    "    y = np.argmax(test_pur[i]) #Make changes here\n",
    "    y_pred.append(y)\n",
    "print(len(y_true))\n",
    "print(len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cl_name = np.array(['Action-0','Action-1','Action-2','Action-3','Action-4','Action-5','Action-6'])\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix')\n",
    "\n",
    "#     print(cm)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "cnf_matrix = confusion_matrix(y_true, y_pred)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=cl_name,\n",
    "                      title='Confusion matrix')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=cl_name, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for transfer learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataa=[]\n",
    "training_labell=[]\n",
    "\n",
    "window=30\n",
    "stride=int(window/2)\n",
    "\n",
    "for k in range(7):\n",
    "    Files=[]\n",
    "    for file in sorted(glob.glob('./ALR_2/Copy_2/1.Distance/Testing/subject_1/Action_{}/*.txt'.format(k))):\n",
    "        Files.append(file)\n",
    "    for m in range(len(Files)):\n",
    "        if m in (0,1):  #unchanged\n",
    "            f = open(Files[m], 'r')\n",
    "            df = f.readlines()\n",
    "            f.close()\n",
    "            p=[]\n",
    "            for i in range(len(df)):\n",
    "                temp = df[i].split()\n",
    "                p.append(temp)\n",
    "            p = np.array(p, 'float32')\n",
    "            \n",
    "            for i in range(0,len(p)-stride, stride):\n",
    "                new_mat = p[i:i+window]\n",
    "                where_are_NaNs = np.isnan(new_mat)\n",
    "                new_mat[where_are_NaNs]=1\n",
    "                new_mat = (new_mat-np.mean(new_mat))/(np.max(new_mat)-np.min(new_mat))\n",
    "                training_dataa.append(new_mat)\n",
    "                training_labell.append(k)\n",
    "   \n",
    "train_data = training_dataa\n",
    "train_label = training_labell\n",
    "print(len(train_data))\n",
    "print(len(train_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(7):\n",
    "    print(i,train_label.count(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_tf = np.array(train_data,'float32')\n",
    "train_data_tf = train_data_tf.reshape(len(train_data_tf),30,136,1)\n",
    "\n",
    "train_label_tf=np.array(train_label) \n",
    "print(train_data_tf.shape)\n",
    "print(train_label_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataa=[]\n",
    "testing_labell=[]\n",
    "\n",
    "window = 30\n",
    "stride = int(window/2)\n",
    "\n",
    "for k in range(7):\n",
    "    Files=[]\n",
    "    for file in sorted(glob.glob('./ALR_2/Copy_2/1.Distance/Testing/subject_1/Action_{}/*.txt'.format(k))):\n",
    "        Files.append(file)\n",
    "    for m in range(len(Files)):\n",
    "        if m in (5,6,7,8,9,15,16,17,18,19):   \n",
    "            print(Files[m])\n",
    "            f = open(Files[m], 'r')\n",
    "            df = f.readlines()\n",
    "            f.close()\n",
    "            p=[]\n",
    "            for i in range(len(df)):\n",
    "                temp = df[i].split()\n",
    "                p.append(temp)\n",
    "            p = np.array(p, 'float32')\n",
    "            \n",
    "            for i in range(0,len(p)-stride, stride):\n",
    "                new_mat = p[i:i+window]\n",
    "                where_are_NaNs = np.isnan(new_mat)\n",
    "                new_mat[where_are_NaNs]=1\n",
    "                new_mat = (new_mat-np.mean(new_mat))/(np.max(new_mat)-np.min(new_mat))\n",
    "                testing_dataa.append(new_mat)\n",
    "                testing_labell.append(k)                          \n",
    "                \n",
    "test_data = testing_dataa\n",
    "test_label = testing_labell\n",
    "\n",
    "print(len(test_data))\n",
    "print(len(test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(7):\n",
    "    print(i,test_label.count(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_tf = np.array(test_data, 'float32') \n",
    "test_data_tf = test_data_tf.reshape((len(test_data_tf),30,136,1))\n",
    "\n",
    "test_label_tf=np.array(test_label)\n",
    "print(test_data_tf.shape)\n",
    "print(test_label_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the categorical labels into one_hot_coding\n",
    "train_label_tf = keras.utils.to_categorical(train_label_tf, 7)\n",
    "test_label_tf = keras.utils.to_categorical(test_label_tf, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_label_tf.shape)\n",
    "print(test_label_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath= 'C:\\\\Users\\\\ma2nq\\\\Desktop\\\\python\\\\Kinect_data\\\\journal\\\\dis.hdf5'\n",
    "os.path.exists(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.load_weights(filepath)\n",
    "test_pur = base_model.predict([test_data_tf], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_1 = 0\n",
    "for i in range(len(test_label_tf)):\n",
    "    if np.argmax(test_label_tf[i]) == np.argmax(test_pur[i]):\n",
    "        match_1+=1\n",
    "accuracy_1 = (match_1/len(test_label_tf))*100\n",
    "print('accuracy :', accuracy_1)\n",
    "print(test_label_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_iter_act = 'C:\\\\Users\\\\ma2nq\\\\Desktop\\\\python\\\\Kinect_data\\\\journal\\\\dis_01.hdf5'\n",
    "os.path.exists(filepath_iter_act)\n",
    "#change the filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Applying Transfer learning, 11 is the  basic\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "for layer in base_model.layers[:10]:\n",
    "        layer.trainable = False\n",
    "for layer in base_model.layers[10:]:  \n",
    "        layer.trainable = True\n",
    "        \n",
    "new_model = Model(inputs=base_model.input, outputs=predictions)  \n",
    "\n",
    "adam = keras.optimizers.Adam(lr= 0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "base_model.compile(loss='categorical_crossentropy', optimizer=adam,  metrics=['accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath_iter_act,\n",
    "                               monitor='val_acc',\n",
    "                               verbose=1, \n",
    "                               save_best_only=True)\n",
    "\n",
    "history = base_model.fit(train_data_tf, train_label_tf, \n",
    "              batch_size=16, \n",
    "              epochs=100, \n",
    "              verbose=1, \n",
    "              validation_data=(test_data_tf, test_label_tf),\n",
    "              callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.load_weights(filepath_iter_act)\n",
    "test_pur = base_model.predict([test_data_tf],batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_1 = 0\n",
    "for i in range(len(test_label_tf)):\n",
    "    if np.argmax(test_label_tf[i]) == np.argmax(test_pur[i]):\n",
    "        match_1+=1\n",
    "accuracy_1 = (match_1/len(test_label_tf))*100\n",
    "print('accuracy :', accuracy_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "for i in range(len(test_label_tf)):\n",
    "    y = np.argmax(test_label_tf[i])\n",
    "    y_true.append(y)\n",
    "\n",
    "for i in range(len(test_pur)):\n",
    "    y = np.argmax(test_pur[i]) #Make changes here\n",
    "    y_pred.append(y)\n",
    "print(len(y_true))\n",
    "print(len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "cl_name = np.array(['Action-0','Action-1','Action-2','Action-3','Action-4','Action-5','Action-6'])\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix')\n",
    "\n",
    "#     print(cm)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "cnf_matrix = confusion_matrix(y_true, y_pred)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=cl_name,\n",
    "                      title='Confusion matrix')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=cl_name, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('C:\\\\Users\\\\ma2nq\\\\Desktop\\\\python\\\\Kinect_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Iterative Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing with 1 or 2 experiments\n",
    "testing_dataa=[]\n",
    "testing_labell=[]\n",
    "\n",
    "window = 30\n",
    "stride = int(window/2)\n",
    "\n",
    "for k in range(7):\n",
    "    Files=[]\n",
    "    \n",
    "    for file in sorted(glob.glob('./ALR_2/Copy_2/1.Distance/Testing/subject_1/Action_{}/*.txt'.format(k))):\n",
    "        Files.append(file)\n",
    "         \n",
    "    for m in range(len(Files)):\n",
    "#         if m in (28,29):\n",
    "#             print(Files[m])\n",
    "            f = open(Files[m], 'r')\n",
    "            df = f.readlines()\n",
    "            f.close()\n",
    "            p=[]\n",
    "            for i in range(len(df)):\n",
    "                temp=df[i].split()\n",
    "                p.append(temp)\n",
    "            p = np.array(p,'float32')\n",
    "\n",
    "            for i in range(0,len(p)-stride, stride):\n",
    "                new_mat = p[i:i+window]\n",
    "                where_are_NaNs = np.isnan(new_mat)\n",
    "                new_mat[where_are_NaNs]=1\n",
    "                new_mat = (new_mat-np.mean(new_mat))/(np.max(new_mat)-np.min(new_mat))        \n",
    "                testing_dataa.append(new_mat)\n",
    "                testing_labell.append(k)\n",
    "    testing_data_234=testing_dataa          \n",
    "    testing_label_234=testing_labell           \n",
    "\n",
    "print(len(testing_dataa))\n",
    "print(len(testing_labell))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(7):\n",
    "    print(testing_label_234.count(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = len(testing_data_234[0])\n",
    "column = len(testing_data_234[0][0])\n",
    "print('row: ', row)\n",
    "print('column: ', column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distance Feature\n",
    "testing_data_dis_ts = np.array(testing_data_234, 'float64')\n",
    "testing_data_dis_ts = testing_data_dis_ts.reshape((len(testing_data_dis_ts),row,column,1))\n",
    "\n",
    "testing_label_ts=np.array(testing_label_234)\n",
    "print(testing_data_dis_ts.shape)\n",
    "print(testing_label_ts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'C:\\\\Users\\\\ma2nq\\\\Desktop\\\\python\\\\Kinect_data\\\\journal\\\\dis_all_01.hdf5'\n",
    "os.path.exists(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.load_weights(filepath)\n",
    "test_pur_234 = base_model.predict([testing_data_dis_ts], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the data labels to categorical\n",
    "# training_label = keras.utils.to_categorical(training_label, 7)\n",
    "testing_label_ts= keras.utils.to_categorical(testing_label_ts, 7)\n",
    "print(testing_label_ts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_1 = 0\n",
    "for i in range(len(testing_label_ts)):\n",
    "    if np.argmax(testing_label_ts[i]) == np.argmax(test_pur_234[i]):\n",
    "        match_1+=1\n",
    "accuracy_1 = (match_1/len(testing_label_ts))*100\n",
    "print('left_hand accuracy:', accuracy_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "for i in range(len(testing_label_ts)):\n",
    "    y = np.argmax(testing_label_ts[i])\n",
    "    y_true.append(y)\n",
    "\n",
    "for i in range(len(test_pur_234)):\n",
    "    y = np.argmax(test_pur_234[i]) #Make changes here\n",
    "    y_pred.append(y)\n",
    "print(len(y_true))\n",
    "print(len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cl_name = np.array(['Action-0','Action-1','Action-2','Action-3','Action-4','Action-5','Action-6'])\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix')\n",
    "\n",
    "#     print(cm)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "cnf_matrix = confusion_matrix(y_true, y_pred)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=cl_name,\n",
    "                      title='Confusion matrix')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=cl_name, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with misclassified samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataa=[]\n",
    "training_labell=[]\n",
    "\n",
    "window=30\n",
    "stride=int(window/2)\n",
    "\n",
    "for k in range(7):\n",
    "    Files=[]\n",
    "    for file in sorted(glob.glob('./ALR_2/Copy_2/1.Distance/Testing/subject_1/Action_{}/*.txt'.format(k))):\n",
    "        Files.append(file)\n",
    "    for m in range(len(Files)):\n",
    "        if m in (5,6,7,8,9,15,16,17,18,19): #unchaged\n",
    "            print(Files[m])\n",
    "            f = open(Files[m], 'r')\n",
    "            df = f.readlines()\n",
    "            f.close()\n",
    "            p=[]\n",
    "            for i in range(len(df)):\n",
    "                temp = df[i].split()\n",
    "                p.append(temp)\n",
    "            p = np.array(p, 'float32')\n",
    "            \n",
    "            for i in range(0,len(p)-stride, stride):\n",
    "                new_mat = p[i:i+window]\n",
    "                where_are_NaNs = np.isnan(new_mat)\n",
    "                new_mat[where_are_NaNs]=1\n",
    "                new_mat = (new_mat-np.mean(new_mat))/(np.max(new_mat)-np.min(new_mat))\n",
    "                training_dataa.append(new_mat)\n",
    "                training_labell.append(k)       \n",
    "\n",
    "train_data_miss = training_dataa\n",
    "train_label_miss = training_labell\n",
    "\n",
    "print(len(train_data_miss))\n",
    "print(len(train_label_miss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(7):\n",
    "    print(i,train_label_miss.count(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_data_miss))\n",
    "print(len(train_label_miss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_pur_234)): ##identified missalpled from 234\n",
    "    if np.argmax(testing_label_ts[i]) != np.argmax(test_pur_234[i]):\n",
    "        x = np.argmax(testing_label_ts[i])\n",
    "        if x in (0,1,2,3,4,5,6):\n",
    "            train_data_miss.append(testing_data_234[i])       \n",
    "            train_label_miss.append(np.argmax(testing_label_ts[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_data_miss))\n",
    "print(len(train_label_miss))\n",
    "# print(len(train_data_miss_NN))\n",
    "# print(len(train_label_miss_NN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Current_BEST MODEL\n",
    "train_data_mis = np.array(train_data_miss,'float32')\n",
    "train_data_mis = train_data_mis.reshape(len(train_data_mis),30,136,1)\n",
    "\n",
    "train_label_mis=np.array(train_label_miss)\n",
    "print(train_data_mis.shape)\n",
    "print(train_label_mis.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the data labels to categorical\n",
    "train_label_mis = keras.utils.to_categorical(train_label_mis, 7)\n",
    "\n",
    "print(train_label_mis.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataa=[]\n",
    "testing_labell=[]\n",
    "\n",
    "window = 30\n",
    "stride = int(window/2)\n",
    "\n",
    "for k in range(7):\n",
    "    Files=[]\n",
    "    for file in sorted(glob.glob('./ALR_2/Copy_2/Coordinate/Testing/subject_1/Action_{}/*.txt'.format(k))):\n",
    "\n",
    "        Files.append(file)\n",
    "    for m in range(len(Files)):    \n",
    "        if m in (5,6,7,8,9,15,16,17,18,19): #unchaged\n",
    "            f = open(Files[m], 'r')\n",
    "#             print(Files[m])\n",
    "            df = f.readlines()\n",
    "            f.close()\n",
    "            p=[]\n",
    "            for i in range(len(df)):\n",
    "                temp=df[i].split()\n",
    "                p.append(temp)\n",
    "            p = np.array(p,'float32')                \n",
    "\n",
    "            for i in range(0,len(p)-stride, stride):\n",
    "                new_mat = p[i:i+window]\n",
    "                where_are_NaNs = np.isnan(new_mat)\n",
    "                new_mat[where_are_NaNs]=1\n",
    "                new_mat = (new_mat-np.mean(new_mat))/(np.max(new_mat)-np.min(new_mat))        \n",
    "                testing_dataa.append(new_mat)\n",
    "                testing_labell.append(k)                                       \n",
    "test_data = testing_dataa\n",
    "test_label = testing_labell\n",
    "print(len(test_data))\n",
    "print(len(test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = len(test_data[0])\n",
    "column = len(test_data[0][0])\n",
    "print(row)\n",
    "print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_mis = np.array(test_data, 'float32')\n",
    "test_data_mis = test_data_mis.reshape((len(test_data_mis),row,column,1))\n",
    "\n",
    "test_label_mis=np.array(test_label)\n",
    "print(test_data_mis.shape)\n",
    "print(test_label_mis.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filepath = 'C:\\\\Users\\\\ma2nq\\\\Desktop\\\\python\\\\Kinect_data\\\\journal\\\\dis.hdf5'\n",
    "os.path.exists(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'C:\\\\Users\\\\ma2nq\\\\Desktop\\\\python\\\\Kinect_data\\\\journal\\\\raw_1.hdf5'\n",
    "os.path.exists(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the data labels to categorical\n",
    "test_label_mis = keras.utils.to_categorical(test_label_mis, 7)\n",
    "\n",
    "print(test_label_mis.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.load_weights(filepath)\n",
    "test_pur_5 = base_model.predict([test_data_mis], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_1 = 0\n",
    "for i in range(len(test_label_mis)):\n",
    "    if np.argmax(test_label_mis[i]) == np.argmax(test_pur_5[i]):\n",
    "        match_1+=1\n",
    "accuracy_1 = (match_1/len(test_label_mis))*100\n",
    "print('accuracy :', accuracy_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing the Model fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_6 = test_pur_5\n",
    "dis_6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dis_1.shape)\n",
    "print(dis_2.shape)\n",
    "print(dis_3.shape)\n",
    "print(dis_4.shape)\n",
    "print(dis_5.shape)\n",
    "print(dis_6.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bal_1 = []\n",
    "for index,i in enumerate(range(len(test_pur_5))):\n",
    "#     print(index, np.argmax(test_label_mis[i]), np.argmax(test_pur_5[i]))\n",
    "    bal_1.append(np.argmax(test_pur_5[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mon = []\n",
    "for i in range(len(dis_1)):\n",
    "    temp = np.vstack((dis_1[i], dis_2[i], dis_3[i],dis_4[i], dis_5[i], dis_6[i],dum[0]))\n",
    "    mon.append(temp)\n",
    "mon = np.array(mon)\n",
    "print(mon.shape)\n",
    "print(mon.shape[0]*mon.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_new_mis = 'C:\\\\Users\\\\ma2nq\\\\Desktop\\\\python\\\\Kinect_data\\\\journal\\\\ang_ras_01234101112131420212223.hdf5'\n",
    "os.path.exists(filepath_new_mis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.load_weights(filepath)\n",
    "test_pur_5 = base_model.predict([test_data_mis],batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_1 = 0\n",
    "for i in range(len(test_label_mis)):\n",
    "    if np.argmax(test_label_mis[i]) == np.argmax(test_pur_5[i]):\n",
    "        match_1+=1\n",
    "accuracy_1 = (match_1/len(test_label_mis))*100\n",
    "print('accuracy :', accuracy_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m = 0\n",
    "bal_1 = []\n",
    "bal_2 = []\n",
    "# bal_3 = []\n",
    "for i in range(len(test_label_mis)):\n",
    "    bal_1.append(test_pur_5[i])\n",
    "    bal_2.append(np.argmax(test_label_mis[i]))\n",
    "#     bal_3.append(np.argmax(test_pur_5[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('Gpr_dis.csv',bal_1, delimiter=',')\n",
    "np.savetxt('Gr_Tr.csv',bal_2, delimiter=',')\n",
    "# np.savetxt('bal_3.csv',bal_3, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "for i in range(len(test_label_mis)):\n",
    "    y = np.argmax(test_label_mis[i])\n",
    "    y_true.append(y)\n",
    "\n",
    "for i in range(len(test_pur_5)):\n",
    "    y = np.argmax(test_pur_5[i]) #Make changes here\n",
    "    y_pred.append(y)\n",
    "print(len(y_true))\n",
    "print(len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "cl_name = np.array(['Action-0','Action-1','Action-2','Action-3','Action-4','Action-5','Action-6'])\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix')\n",
    "\n",
    "#     print(cm)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "cnf_matrix = confusion_matrix(y_true, y_pred)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=cl_name,\n",
    "                      title='Confusion matrix')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=cl_name, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Averaging the model outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'C:\\\\Users\\\\ma2nq\\\\Desktop\\\\python\\\\Kinect_data\\\\journal\\\\Dis_1_01234101112131420212223.hdf5'\n",
    "base_model.load_weights(filepath)\n",
    "distance = base_model.predict([test_data_mis], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'C:\\\\Users\\\\ma2nq\\\\Desktop\\\\python\\\\Kinect_data\\\\journal\\\\Dis_D_1_01234101112131420212223.hdf5'\n",
    "base_model.load_weights(filepath)\n",
    "distance_D = base_model.predict([test_data_mis], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'C:\\\\Users\\\\ma2nq\\\\Desktop\\\\python\\\\Kinect_data\\\\journal\\\\Dis_DD_1_012341011121314202122232425.hdf5'\n",
    "base_model.load_weights(filepath)\n",
    "distance_DD = base_model.predict([test_data_mis], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'C:\\\\Users\\\\ma2nq\\\\Desktop\\\\python\\\\Kinect_data\\\\journal\\\\Ang_1_0123410.hdf5'\n",
    "base_model.load_weights(filepath)\n",
    "angle = base_model.predict([test_data_mis], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'C:\\\\Users\\\\ma2nq\\\\Desktop\\\\python\\\\Kinect_data\\\\journal\\\\Ang_D_1_01234101112131420212223.hdf5'\n",
    "base_model.load_weights(filepath)\n",
    "angle_D = base_model.predict([test_data_mis], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'C:\\\\Users\\\\ma2nq\\\\Desktop\\\\python\\\\Kinect_data\\\\journal\\\\Ang_DD_1_01.hdf5'\n",
    "base_model.load_weights(filepath)\n",
    "angle_DD = base_model.predict([test_data_mis], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on individual model\n",
    "all_list = [distance_n,distance_D_n,distance_DD_n,angle_n,angle_D_n,angle_DD_n]\n",
    "all_list_name = ['distance','distance_D','distance_DD','angle','angle_D','angle_DD']\n",
    "\n",
    "\n",
    "for j in range(len(all_list)):\n",
    "    match_1 = 0\n",
    "    for i in range(len(test_label_mis)):\n",
    "        if np.argmax(test_label_mis[i]) == np.argmax(all_list[j][i]):\n",
    "            match_1+=1\n",
    "    accuracy_1 = (match_1/len(test_label_mis))*100\n",
    "    print(all_list_name[j],':',accuracy_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on individual model\n",
    "all_list = [dis_1,dis_2,dis_3,dis_4,dis_5,dis_6]\n",
    "all_list_name = ['dis_1','dis_2','dis_3','dis_4','dis_5','dis_6']\n",
    "\n",
    "for j in range(len(all_list)):\n",
    "    match_1 = 0\n",
    "    for i in range(len(test_label_mis)):\n",
    "        if np.argmax(test_label_mis[i]) == np.argmax(all_list[j][i]):\n",
    "            match_1+=1\n",
    "    accuracy_1 = (match_1/len(test_label_mis))*100\n",
    "    print(all_list_name[j],':',accuracy_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Prediction ## For 2 sensor FUSION\n",
    "\n",
    "prediction_max = []\n",
    "for i in range(len(test_label_mis)):\n",
    "\n",
    "    x = np.vstack((dis_1[i],dis_2[i],dis_3[i],dis_4[i],dis_5[i],dis_6[i]))\n",
    "\n",
    "    m = np.max(x, axis=0)\n",
    "    prediction_max.append(m)\n",
    "prediction_max = np.array(prediction_max)\n",
    "\n",
    "prediction_avg = []\n",
    "for i in range(len(test_label_mis)):\n",
    "\n",
    "    x = np.vstack((dis_1[i],dis_2[i],dis_3[i],dis_4[i],dis_5[i],dis_6[i]))\n",
    "\n",
    "    x = np.average(x, axis=0)\n",
    "    prediction_avg.append(x)\n",
    "prediction_avg = np.array(prediction_avg)\n",
    "\n",
    "pred_product = []\n",
    "for i in range(len(test_label_mis)):\n",
    "    x = np.vstack((dis_1[i],dis_2[i],dis_3[i],dis_4[i],dis_5[i],dis_6[i]))\n",
    "\n",
    "    o = np.product(x, axis = 0)\n",
    "    pred_product.append(o)\n",
    " \n",
    "pred_product = np.array(pred_product)\n",
    "\n",
    "\n",
    "print(len(prediction_max))\n",
    "print(len(prediction_avg))\n",
    "print(len(pred_product))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on Fusion Method\n",
    "\n",
    "list_1 = [prediction_max, prediction_avg, pred_product]\n",
    "list_2 = ['prediction_max','prediction_avg', 'pred_product']\n",
    "\n",
    "for j in range(len(list_1)):\n",
    "    correct = 0\n",
    "    for i in range(len(test_label_mis)):\n",
    "        if np.argmax(list_1[j][i]) == np.argmax(test_label_mis[i]): #make a change here\n",
    "            correct+=1\n",
    "    accuracy = (correct/test_label_mis.shape[0])*100\n",
    "    print(list_2[j],': {:.2f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for i in range(len(test_label_mis)):\n",
    "    if maj_vote[i] == np.argmax(test_label_mis[i]): #make a change here\n",
    "        correct += 1\n",
    "    accuracy = (correct/test_label_mis.shape[0])*100\n",
    "print('majority voting: {:.2f}'.format(accuracy))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code for majority voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maj_vote = []\n",
    "for i in range(len(test_label_mis)):\n",
    "    v_1 = np.argmax(dis_1[i])\n",
    "    v_2 = np.argmax(dis_2[i])\n",
    "    v_3 = np.argmax(dis_3[i])\n",
    "    v_4 = np.argmax(dis_4[i])\n",
    "    v_5 = np.argmax(dis_5[i])\n",
    "    v_6 = np.argmax(dis_6[i])\n",
    "    lst = [v_1, v_3, v_4, v_5, v_6]\n",
    "\n",
    "    x_1 = np.vstack((dis_1[i],dis_3[i],dis_4[i],dis_5[i],dis_6[i]))\n",
    "    x = list(np.average(x_1, axis=0))\n",
    "    pred = prediction(x,lst)\n",
    "    maj_vote.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(avg,lst):\n",
    "    dic = {}\n",
    "    dhon = []\n",
    "    res = list()\n",
    "    counter = lst\n",
    "    for i in counter:\n",
    "        if not i in res:\n",
    "            res.append(i)\n",
    "    for i in res:\n",
    "        m = counter.count(i)\n",
    "        dic.update({i:m})\n",
    "    pred_key = list(dic.keys())\n",
    "    pred_val = list(dic.values())\n",
    "\n",
    "    val = max(pred_val)    \n",
    "    thres = pred_val.count(val)\n",
    "    if thres ==1:\n",
    "        for output,value in dic.items():\n",
    "            if value == val:\n",
    "                return output\n",
    "    \n",
    "    if thres ==2:\n",
    "        for i in dic.items():\n",
    "            if i[1]==val:\n",
    "                dhon.append(i[0])          \n",
    "        can = max(avg[dhon[0]], avg[dhon[1]])\n",
    "        output = avg.index(can)\n",
    "        return output    \n",
    "    \n",
    "    if thres ==3:\n",
    "        for i in dic.items():\n",
    "            if i[1]==val:\n",
    "                dhon.append(i[0])\n",
    "        can = max(avg[dhon[0]], avg[dhon[1]], avg[dhon[2]])\n",
    "        output = avg.index(can)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_1 = [prediction_max, prediction_avg, pred_product]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(maj_vote)):\n",
    "    if maj_vote[i] == None:\n",
    "        print(i,maj_vote[i], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maj_vote= keras.utils.to_categorical(maj_vote, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "for i in range(len(test_label_mis)):\n",
    "    y = np.argmax(test_label_mis[i])\n",
    "    y_true.append(y)\n",
    "\n",
    "for i in range(len(maj_vote)):\n",
    "    y = np.argmax(maj_vote[i]) #Make changes here\n",
    "    y_pred.append(y)\n",
    "print(len(y_true))\n",
    "print(len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cl_name = np.array(['Action-0','Action-1','Action-2','Action-3','Action-4','Action-5','Action-6'])\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix')\n",
    "\n",
    "#     print(cm)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "cnf_matrix = confusion_matrix(y_true, y_pred)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=cl_name,\n",
    "                      title='Confusion matrix')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=cl_name, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_identical_lists(A, B, C, D):\n",
    "    m = 0\n",
    "    for i in range(len(list_1)):\n",
    "        if A[i] == B[i]==C[i] == D[i]:\n",
    "            m+=1\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_identical_lists([0, 0, 0], [0, 0, 0], [2, 1, 0], [2, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [0,1,2,3,4,0]\n",
    "a.count(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_identical_lists(lst1, lst2, lst3, lst4):\n",
    "    a = [lst1, lst2, lst3, lst4]\n",
    "    return len([i for i in a if a.count(i) > 1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_identical_lists(lst1, lst2, lst3, lst4):\n",
    "\tlists = [lst1, lst2, lst3, lst4]\n",
    "\trepeat_lists = []\n",
    "\t\n",
    "\tfor x in lists:\n",
    "\t\tcount = lists.count(x)\n",
    "\t\tif count > 1:\n",
    "\t\t\tprint(x)\n",
    "\t\t\trepeat_lists.append(x)\n",
    "\treturn len(repeat_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_identical_lists(lst1, lst2, lst3, lst4):\n",
    "\tlst = [lst1, lst2, lst3, lst4]\n",
    "\tfor i in lst:\n",
    "\t\tif lst.count(i) > 1:\n",
    "\t\t\treturn lst.count(i)\n",
    "\t\telif (lst.count(i) == 1 for i in lst):\n",
    "\t\t\treturn 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_PIN(pin):\n",
    "    string = str(pin)\n",
    "    if len(string)==4 or len(string)==6:\n",
    "        return('valid')\n",
    "    else:\n",
    "        return('not valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_valid_PIN(1323)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_PIN(pin):\n",
    "\treturn len(str(pin)) in [4, 6] and str(pin).isdigit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_valid_PIN(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_of_caps(word):\n",
    "    return [word.index(i)+1 for i in word if i.isupper()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equal(a, b, c):\n",
    "    lst = [a,b,c]\n",
    "    for i in lst:\n",
    "        if lst.count(i)>1:\n",
    "            return lst.count(i)\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_vowels(\"I have never seen a thin person drinking Diet Coke.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_vowels(txt):\n",
    "    return ''.join([x for x in txt if x not in 'aeiouAEIOU'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_dups([\"John\", \"Taylor\", \"John\"])  [\"John\", \"Taylor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_dups(txt):\n",
    "    res = []\n",
    "    for i in txt:\n",
    "        if i not in res:\n",
    "            res.append(i) \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_dups([\"John\", \"Taylor\", \"John\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_two_smallest_nums([19, 5, 42, 2, 77])  7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_two_smallest_nums(txt):\n",
    "    m = sorted(txt)\n",
    "    return(sum(m[:2]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_neg(txt):\n",
    "    return([sum(i) for i in txt if i <= 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_neg([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, -11, -12, -13, -14, -15])\n",
    "# There are a total of 10 positive numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, -11, -12, -13, -14, -15]\n",
    "def sum_neg(lst):\n",
    "    return [sum(1 for x in lst if x>=0), sum(x for x in lst if x<0) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numInStr([\"1a\", \"20\", \"10\", \"10\"])  [\"1a\", \"2b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [\"20\", \"10\", \"10\"]\n",
    "[all(i.isnumeric() for i in lst)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return [i for i in lst if any(j.isnumeric() for j in i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numInStr(lst):\n",
    "    ress = []\n",
    "    for i in lst:\n",
    "        nm = str(i)\n",
    "        res = []\n",
    "        for j in range(len(nm)):\n",
    "            res.append(is_int(nm[j]))\n",
    "        if True in res:\n",
    "            ress.append(nm)\n",
    "    return ress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'102'.isnumeric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab = 'abc'\n",
    "def is_int(ab):\n",
    "    try:\n",
    "        ab = int(ab)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_int(ab):\n",
    "    try: \n",
    "        int(ab)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {}\n",
    "res = list()\n",
    "counter = ['B','B','A','B','C','A','B','B','A','C']\n",
    "for i in counter:\n",
    "    if not i in res:\n",
    "        res.append(i)\n",
    "for i in res:\n",
    "    m = counter.count(i)\n",
    "    dic.update({i:m})\n",
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_frequencies([\"A\", \"B\", \"A\", \"A\", \"A\"])  { \"A\" : 4, \"B\" : 1 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequencies(lst):\n",
    "    dic = {}\n",
    "    lst = list(set(lst))\n",
    "    dic.update({i, lst.count(i)} for i in lst)\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_frequencies([\"A\", \"B\", \"A\", \"A\", \"A\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Create a function that takes an array of numbers and return \"Boom!\"\n",
    "if the number 7 appears in the array. Otherwise, return \"there is no 7 in the array\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [\"A\", \"B\", \"A\", \"107\", \"A\"]\n",
    "def seven_boom(lst):\n",
    "    return 'Boom!' if '7' in str(lst) else 'there is no 7 in the list'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmix(\"123456\")  \"214365\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"hTsii  s aimex dpus rtni.g\"\n",
    "def unmix(a):\n",
    "    lst = []\n",
    "    for i in range(0,len(a),2):\n",
    "        var = a[i:i+2]\n",
    "        var = var[::-1]\n",
    "        lst.append(var)\n",
    "    return ''.join(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmix(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_1 = [\"A\", \"A\", \"A\"]\n",
    "arr_2 = [\"B\"]\n",
    "# shu_arr= [\"A\", \"B\", \"A\", \"B\", \"A\", \"B\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bridge_shuffle(lst1, lst2):\n",
    "    step = 1\n",
    "    for x in lst2:\n",
    "        lst1.insert(step,x)\n",
    "        step += 2\n",
    "    return lst1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bridge_shuffle(arr_1, arr_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleStr = \"Hello!!\" \n",
    "for elem in sampleStr:\n",
    "    print(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Two ice cream stands: A and B each occupy a spot on the beach, from [0, 100]. Their positions are represented \n",
    "with coordinates (A, B). One position could be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profit(A, B):\n",
    "    assert A<B, 'A is greater than B'\n",
    "    prof_A = 0\n",
    "    prof_B = 0\n",
    "    for i in range(0,100+1):\n",
    "        if abs(A-i)<abs(B-i):\n",
    "            prof_A+=1\n",
    "        else :\n",
    "            prof_B +=1\n",
    "    return prof_A, prof_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_before_second(\"a rabbit jumps joyfully\", \"a\", \"j\")  True\n",
    "# Every instance of \"a\" occurs before every instance of \"j\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"a rabbit jumps joyfully\"\n",
    "def first_before_second(txt, a, b):\n",
    "    lst = []\n",
    "    for i in txt.split(' '):\n",
    "        if 'j' in i:\n",
    "            lst.append('j')\n",
    "        if 'a' in i:\n",
    "            lst.append('a')\n",
    "    a = lst.count('a')\n",
    "    b = lst.count('j')\n",
    "    j = 0\n",
    "    for i in lst:\n",
    "        if i =='a':\n",
    "            j+=1\n",
    "        if i == 'j':\n",
    "            break\n",
    "    if a==j:\n",
    "        return 'True'\n",
    "    else:\n",
    "        return 'False'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_before_second(s, first, second):\n",
    "    return s.rfind(first) < s.find(second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'CatBatSatMatGate'\n",
    "if word.rfind('atee') != -1:\n",
    "    print(True)\n",
    "else:\n",
    "    print(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [(2,3),(1,2),(7,3)]\n",
    "b = [(5,8),(3,4),(5,8),(3,4)]\n",
    "a = [np.array(i) for i in a]\n",
    "b = [np.array(i) for i in b]\n",
    "\n",
    "min([np.linalg.norm(i-j) for i in a for j in b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_dis(a,b):\n",
    "    a = [np.array(i) for i in a]\n",
    "    b = [np.array(i) for i in b]\n",
    "    minimum = min([np.linalg.norm(i-j) for i in a for j in b])\n",
    "    return minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [(2,3),(1,2)]\n",
    "b = [(5,8),(3,4),(5,8),(3,4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dis(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def happy(n):\n",
    "    while n!=4 and n!=1:\n",
    "        n = sum([int(i)**2 for i in str(n)])\n",
    "    return True if n == 1 else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flip_list([1, 2, 3, 4])  [[1], [2], [3], [4]]\n",
    "# Take a horizontal list and flip it vertical.\n",
    "\n",
    "flip_list([[5], [6], [9]])  [5, 6, 9]\n",
    "# Take a vertical list and flip it horizontal.\n",
    "\n",
    "flip_list([])  []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_list(lst):\n",
    "    if lst==[]:\n",
    "        return lst\n",
    "    elif isinstance(lst[0], list):\n",
    "        return [l[0] for l in lst]\n",
    "    else:\n",
    "        return [[l] for l in lst]\n",
    "\n",
    "flip_list(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_list(lst):\n",
    "    if lst == []:\n",
    "        return lst\n",
    "    elif isinstance(lst[0], list):\n",
    "        return [l[0] for l in lst]\n",
    "    else:\n",
    "        return [[l]  for l in lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = [1, 2, 3]\n",
    "\n",
    "result = isinstance(numbers, dict)\n",
    "print(numbers,'instance of dict?', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_best_student({\n",
    "  \"John\": [100, 90, 80],\n",
    "  \"Bob\": [100, 70, 80]\n",
    "})  \"John\"\n",
    "\n",
    "\n",
    "# Bob's avg = 83.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {\"John\": [100, 90, 80], \"Bobbb\": [100, 100, 100]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_best_student(dic):\n",
    "    return sorted(dic, key =lambda x:np.average(x[1]), reverse = True)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_student(dic):\n",
    "    return sorted(dic, key =lambda x:np.average(x[1]), reverse = True)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_best_student(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "# Create a deque\n",
    "DoubleEnded = collections.deque([\"Mon\",\"Tue\",\"Wed\"])\n",
    "print (DoubleEnded)\n",
    "\n",
    "# Append to the right\n",
    "print(\"Adding to the right: \")\n",
    "DoubleEnded.append(\"Thu\")\n",
    "print (DoubleEnded)\n",
    "\n",
    "# append to the left\n",
    "print(\"Adding to the left: \")\n",
    "DoubleEnded.appendleft(\"Sun\")\n",
    "print (DoubleEnded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(DoubleEnded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './s_Tuhin/t_Handl_31/kinect/'\n",
    "# file_path = './data/s_Zpheng/t_Handl_1/kinect/data.p'\n",
    "# C:\\Users\\alami\\Desktop\\Myo_Paper\\DATA_NEW\\s_Al-Amin_1\n",
    "# C:\\Users\\ma2nq\\Desktop\\python\\Kinect_data\\s_Tuhin\\t_Handl_31\\kinect\n",
    "# C:\\Users\\ma2nq\\Desktop\\python\\Kinect_data\\s_Tuhin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_idx = 0\n",
    "while True:\n",
    "    try:        \n",
    "        img = cv2.imread(file_path + str(img_idx) + '.jpg')\n",
    "        cv2.putText(img, 'Help', (10, 230), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255))\n",
    "        cv2.putText(img, 'f: forward, r: reverse, q: quit', (10, 260), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255))\n",
    "        cv2.imshow('img', img)\n",
    "        k = cv2.waitKey(0)\n",
    "        if k == ord(' ') or k == ord('f'):\n",
    "            img_idx += 1\n",
    "        elif k == ord('r'): \n",
    "            img_idx -= 1\n",
    "        elif k == ord('q'):\n",
    "            cv2.destroyAllWindows()     \n",
    "            break        \n",
    "    except:\n",
    "        cv2.destroyAllWindows()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.random([1,7])\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Majority Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight = np.random.random((6,7))\n",
    "# weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
